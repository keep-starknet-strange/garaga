#!/usr/bin/env python3
"""
Garaga Test Profiling Tool

This script automates the process of running snforge tests with trace data collection,
generating individual profile data using cairo-profiler, creating performance
visualizations using pprof, and organizing the results.

Usage:
    python tools/profile_tests.py [test_name_filter]
    python tools/profile_tests.py --all

Examples:
    python tools/profile_tests.py msm_BN254_1P    # Run specific test
    python tools/profile_tests.py --all           # Run all tests
    python tools/profile_tests.py                 # Run all tests (same as --all)
"""

import argparse
import json
import os
import re
import subprocess
import sys
from abc import ABC, abstractmethod
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from datetime import datetime
from functools import wraps
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd
from rich.align import Align
from rich.console import Console
from rich.panel import Panel
from rich.progress import (
    BarColumn,
    Progress,
    SpinnerColumn,
    TaskProgressColumn,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
)
from rich.text import Text
from tabulate import tabulate

from tools.process_manager import ProcessManager, managed_subprocess

# Global state
console = Console()


# Configuration & Constants
@dataclass
class Config:
    """Consolidated configuration for the profiling tool."""

    contracts: List[str] = field(
        default_factory=lambda: [
            "autogenerated/groth16_example_bn254",
            "autogenerated/groth16_example_bls12_381",
            "autogenerated/risc0_verifier_bn254",
            "autogenerated/sp1_verifier_bn254",
            "autogenerated/noir_ultra_keccak_honk_example",
            "autogenerated/noir_ultra_keccak_zk_honk_example",
            "autogenerated/noir_ultra_starknet_honk_example",
            "autogenerated/noir_ultra_starknet_zk_honk_example",
        ]
    )

    excluded_profile_samples: set = field(
        default_factory=lambda: {"memory holes", "casm size", "calls"}
    )

    sierra_gas_weights: Dict[str, int] = field(
        default_factory=lambda: {
            "steps": 100,
            "range_check": 70,
            "range_check96": 56,
            "keccak": 136189,
            "pedersen": 4050,
            "bitwise": 583,
            "ecop": 4085,
            "poseidon": 491,
            "add_mod": 230,
            "mul_mod": 604,
        }
    )


@dataclass
class Paths:
    """Path management with validation."""

    workspace_root: Path

    def __post_init__(self):
        self.src_dir = self.workspace_root / "src"
        self.trace_dir = self.src_dir / "snfoundry_trace"
        self.docs_benchmarks = self.workspace_root / "docs" / "benchmarks"
        self.readme = self.workspace_root / "README.md"

    def ensure_dirs(self):
        """Ensure necessary directories exist."""
        self.docs_benchmarks.mkdir(parents=True, exist_ok=True)


# Logging & UI
class Logger:
    """Centralized logging with decorators and context management."""

    STYLES = {
        "info": ("ℹ️", "[dim]{message}[/dim]"),
        "success": ("✅", "[green]{message}[/green]"),
        "warning": ("⚠️", "[yellow]{message}[/yellow]"),
        "error": ("❌", "[bold red]{message}[/bold red]"),
        "debug": ("🔍", "[dim cyan]{message}[/dim cyan]"),
    }

    @staticmethod
    def log(message: str, level: str = "info", emoji: str = None):
        """Consolidated logging function."""
        if level == "debug" and not console.is_terminal:
            return

        emoji_char, style_template = Logger.STYLES.get(level, Logger.STYLES["info"])
        if emoji:
            emoji_char = emoji

        formatted_message = style_template.format(message=message)
        console.print(f"{emoji_char} {formatted_message}")

    @staticmethod
    def header(title: str):
        """Print formatted header."""
        console.print(
            Panel(
                Align.center(Text(title, style="bold cyan")),
                border_style="cyan",
                padding=(0, 1),
                width=80,
            )
        )

    @staticmethod
    def exit_panel(message: str, status: str, border_style: str = "green"):
        """Show standardized exit panel."""
        console.print(
            Panel(
                Align.center(message),
                border_style=border_style,
                title=f"[{border_style}]{status}[/{border_style}]",
            )
        )


def handle_errors(func):
    """Decorator for centralized error handling - only for non-critical functions."""

    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            Logger.log(f"Error in {func.__name__}: {e}", level="error")
            return False

    return wrapper


def fail_on_error(func):
    """Decorator that prints errors and exits immediately."""

    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            import traceback

            Logger.log(f"FATAL ERROR in {func.__name__}: {e}", level="error")
            Logger.log(f"Full traceback:\n{traceback.format_exc()}", level="error")
            Logger.exit_panel(f"💥 Fatal error in {func.__name__}: {e}", "Error", "red")
            sys.exit(1)

    return wrapper


def log_step(description: str):
    """Decorator for logging method execution steps."""

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            Logger.log(f"Starting: {description}", level="info")
            result = func(*args, **kwargs)
            Logger.log(f"Completed: {description}", level="success")
            return result

        return wrapper

    return decorator


class UIBuilder:
    """Builder for console UI elements."""

    @staticmethod
    def progress(advanced: bool = False) -> Progress:
        """Create progress bar."""
        base_elements = [
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
        ]
        if advanced:
            return Progress(
                *base_elements,
                BarColumn(bar_width=40, style="cyan", complete_style="green"),
                "[progress.percentage]{task.percentage:>3.0f}%",
                "•",
                TextColumn("({task.completed}/{task.total} files)"),
                "•",
                TimeElapsedColumn(),
                "•",
                TimeRemainingColumn(),
                console=console,
                transient=False,
            )
        return Progress(
            *base_elements,
            BarColumn(bar_width=30, style="cyan", complete_style="green"),
            TaskProgressColumn(),
            "•",
            TimeElapsedColumn(),
            "•",
            TimeRemainingColumn(),
            console=console,
            transient=False,
        )


# Enhanced TestResourceInfo
@dataclass
class TestResourceInfo:
    """Container for test resource information with enhanced methods."""

    test_name_hierarchical: str
    test_name_simple: str
    steps: int
    builtins: dict = field(default_factory=dict)
    image_path: str = ""
    category: str = "garaga"
    sierra_gas: int = field(init=False)

    def __post_init__(self):
        self.sierra_gas = self._calculate_sierra_gas()
        if self.category == "garaga" and "::" in self.test_name_hierarchical:
            if not self.test_name_hierarchical.startswith("garaga::"):
                self.category = "contracts"

    @classmethod
    def from_trace_file(
        cls,
        test_name: str,
        trace_path: Path,
        profile_path: Path,
        resources: dict,
        image_path: str = "",
    ):
        """Create instance from trace file data."""
        steps = resources.get("steps", 0)
        builtins = {k: v for k, v in resources.items() if k != "steps"}
        test_name_simple = test_name.split("::")[-1]

        return cls(
            test_name_hierarchical=test_name,
            test_name_simple=test_name_simple,
            steps=steps,
            builtins=builtins,
            image_path=image_path,
            category="contracts" if not test_name.startswith("garaga::") else "garaga",
        )

    @staticmethod
    def predict_trace_path(test_name: str, base_dir: Path) -> Path:
        """Predict trace file path from hierarchical test name."""
        return base_dir / f"{test_name.replace('::', '_')}.json"

    @staticmethod
    def extract_hierarchy_parts(test_name: str) -> List[str]:
        """Extract module hierarchy parts."""
        parts = test_name.split("::")
        if parts and parts[-1].startswith("test_"):
            parts = parts[:-1]
        if parts and parts[0] == "garaga":
            parts = parts[1:]
        return parts if parts else ["tests"]

    def _calculate_sierra_gas(self) -> int:
        """Calculate Sierra gas using weight table."""
        config = Config()
        total = self.steps * config.sierra_gas_weights["steps"]
        for builtin, count in self.builtins.items():
            if builtin in config.sierra_gas_weights:
                total += count * config.sierra_gas_weights[builtin]
        return total

    def to_benchmark_row(self) -> dict:
        """Convert to benchmark row format."""
        result = {
            "test_name_hierarchical": self.test_name_hierarchical,
            "test_name": self.test_name_simple,
            "steps": self.steps,
            "sierra_gas": self.sierra_gas,
            "image_path": self.image_path,
            "category": self.category,
        }

        # Add builtin columns
        config = Config()
        for builtin in config.sierra_gas_weights.keys():
            if builtin != "steps":
                result[builtin] = self.builtins.get(builtin, 0)

        return result


# Data Processing
class JsonManager:
    """Utility class for JSON operations with context management."""

    @staticmethod
    @handle_errors
    def read(file_path: Path, default=None):
        """Read JSON file with error handling."""
        if not file_path.exists():
            return default if default is not None else {}
        with open(file_path, "r") as f:
            return json.load(f)

    @staticmethod
    @handle_errors
    def write(file_path: Path, data, indent: int = 2) -> bool:
        """Write JSON file with error handling."""
        with open(file_path, "w") as f:
            json.dump(data, f, indent=indent)
        return True


class TraceProcessor:
    """Strategy for processing trace files."""

    def __init__(self, config: Config):
        self.config = config
        self.sample_mapping = {
            "steps": "steps",
            "range check builtin": "range_check",
            "range check96 builtin": "range_check96",
            "keccak builtin": "keccak",
            "pedersen builtin": "pedersen",
            "bitwise builtin": "bitwise",
            "ecop builtin": "ecop",
            "poseidon builtin": "poseidon",
            "add mod builtin": "add_mod",
            "mul mod builtin": "mul_mod",
        }

    def process(
        self, test_name: str, trace_path: Path
    ) -> Tuple[bool, str, str, Optional[TestResourceInfo]]:
        """Process a single trace file."""
        try:
            with managed_subprocess() as (run_cmd, pm):
                profile_file = self._generate_profile(trace_path, run_cmd)
                if not profile_file or not profile_file.exists():
                    return False, test_name, "", None

                resources = self._extract_resources(profile_file, run_cmd)
                test_name_simple = test_name.split("::")[-1]
                docs_image_path = (
                    Paths(Path.cwd()).docs_benchmarks / f"{test_name_simple}.png"
                )
                image_success = self._generate_image(
                    profile_file, docs_image_path, run_cmd
                )

                test_info = TestResourceInfo.from_trace_file(
                    test_name,
                    trace_path,
                    profile_file,
                    resources,
                    f"docs/benchmarks/{test_name_simple}.png",
                )

                return image_success, test_name, test_info.image_path, test_info
        except Exception as e:
            # Re-raise with more context
            raise Exception(f"Failed to process {test_name}: {str(e)}") from e

    def _generate_profile(self, trace_file: Path, run_cmd) -> Optional[Path]:
        """Generate profile from trace file."""
        profile_file = trace_file.parent / f"{trace_file.stem}.pb.gz"
        result = run_cmd(
            [
                "cairo-profiler",
                "build-profile",
                str(trace_file),
                "--output-path",
                str(profile_file),
            ],
            check=True,
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            raise Exception(f"cairo-profiler build-profile failed: {result.stderr}")
        return profile_file

    def _extract_resources(self, profile_file: Path, run_cmd) -> dict:
        """Extract resources from profile file."""
        result = run_cmd(
            ["cairo-profiler", "view", str(profile_file), "--list-samples"],
            check=True,
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            raise Exception(
                f"cairo-profiler view --list-samples failed: {result.stderr}"
            )

        samples = [
            line.strip()
            for line in result.stdout.strip().split("\n")
            if line.strip() and line.strip() not in self.config.excluded_profile_samples
        ]

        resources = {}
        for sample in samples:
            try:
                sample_result = run_cmd(
                    ["cairo-profiler", "view", str(profile_file), "--sample", sample],
                    check=True,
                    capture_output=True,
                    text=True,
                )

                # Updated regex to handle the new cairo-profiler output format
                # Example: "Showing nodes accounting for 11582 steps, 94.63% of 12239 steps total"
                pattern = rf"(\d+) {re.escape(sample)} total"
                match = re.search(pattern, sample_result.stdout, re.IGNORECASE)
                if match:
                    count = int(match.group(1))
                    normalized = self.sample_mapping.get(sample.lower().strip())
                    if normalized:
                        resources[normalized] = count
            except subprocess.CalledProcessError as e:
                # Log individual sample errors but continue
                Logger.log(f"Failed to extract sample '{sample}': {e}", level="warning")
                continue

        return resources

    def _generate_image(self, profile_file: Path, output_path: Path, run_cmd) -> bool:
        """Generate PNG image using pprof."""
        result = run_cmd(
            [
                "go",
                "tool",
                "pprof",
                "-png",
                "-sample_index=steps",
                "-output",
                str(output_path),
                str(profile_file),
            ],
            check=True,
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            raise Exception(f"pprof failed: {result.stderr}")
        return True


# Command Pattern Implementation
class Command(ABC):
    """Abstract base class for commands."""

    @abstractmethod
    def execute(self) -> bool:
        """Execute the command."""

    @abstractmethod
    def can_execute(self) -> bool:
        """Check if command can be executed."""


class CheckDependenciesCommand(Command):
    """Command to check tool dependencies."""

    def __init__(self, config: Config):
        self.config = config
        self.tools = [
            ("snforge", "Starknet Foundry", ["snforge", "--version"]),
            ("cairo-profiler", "cairo-profiler", ["cairo-profiler", "--version"]),
            ("go", "Go to use pprof", ["go", "version"]),
        ]

    @log_step("Checking Dependencies")
    @handle_errors
    def execute(self) -> bool:
        Logger.header("🔍 Checking Dependencies")
        with managed_subprocess() as (run_cmd, pm):
            for tool, desc, cmd in self.tools:
                try:
                    result = run_cmd(cmd, check=True, capture_output=True, text=True)
                    Logger.log(f"{tool}: {result.stdout.strip()}", level="success")
                except subprocess.CalledProcessError:
                    Logger.log(
                        f"{tool} not found. Please install {desc}.", level="error"
                    )
                    return False

        Logger.log("All dependencies satisfied!", level="success")
        return True

    def can_execute(self) -> bool:
        return True


class RunTestsCommand(Command):
    """Command to run tests with trace collection."""

    def __init__(self, paths: Paths, test_filter: Optional[str] = None):
        self.paths = paths
        self.test_filter = test_filter
        self.passed_tests = []

    @log_step("Running Tests & Collecting Traces")
    @handle_errors
    def execute(self) -> bool:
        Logger.header("🧪 Running Tests & Collecting Traces")

        original_cwd = os.getcwd()
        cmd = ["snforge", "test"]
        if self.test_filter:
            cmd.append(self.test_filter)
        cmd.append("--save-trace-data")

        test_dirs = [self.paths.src_dir] + self._get_contract_paths()
        success_count = 0

        with managed_subprocess() as (run_cmd, pm):
            with UIBuilder.progress() as progress:
                # Create a single task outside the loop
                base_desc = (
                    f"Running tests matching: {self.test_filter}"
                    if self.test_filter
                    else "Running all tests"
                )
                task = progress.add_task(base_desc, total=len(test_dirs))

                for i, test_dir in enumerate(test_dirs):
                    # Update the description for the current directory
                    current_desc = f"{base_desc} from {test_dir.relative_to(self.paths.workspace_root)}"
                    progress.update(task, description=current_desc)

                    if not test_dir.exists() or not (test_dir / "Scarb.toml").exists():
                        continue
                    os.chdir(test_dir)
                    result = run_cmd(cmd, capture_output=True, text=True)

                    if result.returncode == 0:
                        success_count += 1
                        passed = re.findall(
                            r"\[PASS\]\s+([^\s]+)\s+\(l1_gas:", result.stdout
                        )
                        self.passed_tests.extend(passed)
                    else:
                        raise Exception(f"Failed to run tests: {result.stderr}")
                    # Advance the progress by 1 for each directory processed
                    progress.advance(task, 1)

        os.chdir(original_cwd)

        if success_count > 0:
            Logger.log(f"Found {len(self.passed_tests)} passed tests", level="success")
            return True
        else:
            Logger.log("No tests executed successfully", level="error")
            return False

    def can_execute(self) -> bool:
        return self.paths.src_dir.exists()

    def _get_contract_paths(self) -> List[Path]:
        """Get contract paths from config."""
        config = Config()
        return [
            self.paths.workspace_root / "src" / "contracts" / contract
            for contract in config.contracts
        ]


class ProcessTracesCommand(Command):
    """Command to process trace files and generate profiles."""

    def __init__(self, paths: Paths, passed_tests: List[str], parallel_jobs: int = 4):
        self.paths = paths
        self.passed_tests = passed_tests
        self.parallel_jobs = parallel_jobs
        self.test_resources = []

    @log_step("Processing Traces & Extracting Resources")
    def execute(self) -> bool:
        if not self.passed_tests:
            Logger.log("No passed tests to process", level="warning")
            return False

        Logger.header(
            f"🚀 Processing {len(self.passed_tests)} tests with {self.parallel_jobs} parallel jobs"
        )

        trace_files = self._find_trace_files()
        if not trace_files:
            Logger.log("No trace files found", level="warning")
            return False

        processor = TraceProcessor(Config())

        try:
            with (
                ProcessManager() as pm,
                ThreadPoolExecutor(max_workers=self.parallel_jobs) as executor,
            ):
                futures = {
                    executor.submit(processor.process, test_name, trace_path): test_name
                    for test_name, trace_path, _ in trace_files
                }

                success_count = self._process_futures_interruptible(futures, pm)

            if self.test_resources:
                self._update_summary()

            Logger.log(
                f"Successfully processed {success_count}/{len(trace_files)} tests",
                level="success",
            )
            return success_count > 0

        except KeyboardInterrupt:
            Logger.log("Processing interrupted by user", level="warning")
            raise

    def can_execute(self) -> bool:
        return len(self.passed_tests) > 0

    def _find_trace_files(self) -> List[Tuple[str, Path, Path]]:
        """Find trace files for passed tests."""
        found = []
        trace_dirs = self._get_trace_directories()

        for test_name in self.passed_tests:
            for trace_dir in trace_dirs:
                if not trace_dir.exists():
                    continue
                predicted_path = TestResourceInfo.predict_trace_path(
                    test_name, trace_dir
                )
                if predicted_path.exists():
                    found.append((test_name, predicted_path, trace_dir.parent))
                    break

        return found

    def _get_trace_directories(self) -> List[Path]:
        """Get all potential trace directories."""
        dirs = [self.paths.trace_dir]
        config = Config()
        for contract in config.contracts:
            dirs.append(
                self.paths.workspace_root
                / "src"
                / "contracts"
                / contract
                / "snfoundry_trace"
            )
        return dirs

    def _process_futures_interruptible(self, futures, pm) -> int:
        """Process completed futures with progress tracking and interruptibility."""
        success_count = 0

        with UIBuilder.progress(True) as progress:
            task = progress.add_task("Processing tests", total=len(futures))

            try:
                for future in futures:
                    if pm.shutdown_event.is_set():
                        future.cancel()
                        continue

                    try:
                        success, test_name, _, test_info = future.result(
                            timeout=60 * 20
                        )  # Increased timeout
                        if success and test_info:
                            self.test_resources.append(test_info)
                            console.print(f"✨ [green]{test_name}[/green]")
                            success_count += 1
                        else:
                            console.print(
                                f"💥 [red]{test_name}[/red]: Processing failed"
                            )
                    except Exception as e:
                        test_name = futures[future]
                        error_msg = str(e)
                        console.print(f"💥 [red]{test_name}[/red]: {error_msg}")

                        # Print full error details and exit
                        import traceback

                        Logger.log(
                            f"FATAL ERROR processing {test_name}: {error_msg}",
                            level="error",
                        )
                        Logger.log(
                            f"Full traceback:\n{traceback.format_exc()}", level="error"
                        )
                        Logger.exit_panel(
                            f"💥 Fatal error processing {test_name}: {error_msg}",
                            "Error",
                            "red",
                        )
                        sys.exit(1)

                    progress.advance(task, 1)

            except KeyboardInterrupt:
                # Cancel all remaining futures immediately
                for future in futures:
                    future.cancel()
                # Re-raise to propagate the interrupt
                raise

        return success_count

    def _update_summary(self):
        """Update global summary JSON."""
        summary_file = self.paths.docs_benchmarks / "test_summary.json"
        summary_data = JsonManager.read(summary_file, {})

        timestamp_iso = datetime.now().isoformat()
        for test_info in self.test_resources:
            summary_data[test_info.test_name_simple] = {
                "last_updated": timestamp_iso,
                "latest_metrics": test_info.to_benchmark_row(),
            }

        if JsonManager.write(summary_file, summary_data):
            Logger.log("Performance data saved", level="success")


class GenerateBenchmarksCommand(Command):
    """Command to generate Cairo benchmarks."""

    def __init__(self, paths: Paths):
        self.paths = paths

    @log_step("Generating Cairo Benchmarks")
    def execute(self) -> bool:
        Logger.log("Generating Cairo benchmarks...", level="info")

        summary_file = self.paths.docs_benchmarks / "test_summary.json"
        test_data = JsonManager.read(summary_file, {})

        if not test_data:
            Logger.log("No test data available", level="warning")
            return False

        success = self._update_readme(test_data)
        if success:
            Logger.log(
                f"Generated benchmarks for {len(test_data)} tests", level="success"
            )

        return success

    def can_execute(self) -> bool:
        return self.paths.readme.exists()

    @handle_errors
    def _update_readme(self, test_data: dict) -> bool:
        """Update README with benchmarks."""
        with open(self.paths.readme, "r", encoding="utf-8") as f:
            content = f.read()

        start_idx, end_idx = self._find_benchmarks_section(content)
        lines = content.split("\n")

        new_content = self._generate_benchmarks_content(test_data)
        new_lines = new_content.rstrip().split("\n")

        if start_idx is not None:
            if end_idx < len(lines) and lines[end_idx - 1].strip() == "":
                updated_lines = lines[:start_idx] + new_lines + lines[end_idx:]
            else:
                updated_lines = lines[:start_idx] + new_lines + [""] + lines[end_idx:]
        else:
            support_idx = next(
                (
                    i
                    for i, line in enumerate(lines)
                    if "Support & How to Contribute" in line
                ),
                None,
            )
            if support_idx:
                updated_lines = (
                    lines[:support_idx] + new_lines + [""] + lines[support_idx:]
                )
            else:
                updated_lines = lines + [""] + new_lines

        with open(self.paths.readme, "w", encoding="utf-8") as f:
            f.write("\n".join(updated_lines))

        Logger.log("Updated README.md with benchmarks", level="success")
        return True

    def _find_benchmarks_section(
        self, content: str
    ) -> Tuple[Optional[int], Optional[int]]:
        """Find Cairo Benchmarks section."""
        lines = content.split("\n")
        start_idx = end_idx = None

        for i, line in enumerate(lines):
            if line.strip() == "## Cairo Benchmarks":
                start_idx = i
            elif (
                start_idx is not None
                and line.startswith("## ")
                and line.strip() != "## Cairo Benchmarks"
            ):
                end_idx = i
                break

        if start_idx is not None and end_idx is None:
            end_idx = len(lines)

        return start_idx, end_idx

    def _generate_benchmarks_content(self, test_data: dict) -> str:
        """Generate complete benchmarks content."""
        if not test_data:
            return "## Cairo Benchmarks\n\n*No benchmark data available.*\n\n"

        report_builder = ReportBuilder(test_data)
        content = report_builder.build()

        return f"""## Cairo Benchmarks

📊 **Click on any section below to expand and view detailed benchmark tables with test performance metrics.**

**Note:** Click on the test name to view the profiling image. Resources shown in the table for contracts include extra costs due to the foundry logic.
Check the profile file for more details.

{content.rstrip()}

---
🔄 **To regenerate these benchmarks:** Run `make profile-test` from the project root.

"""


# Report Generation with Builder Pattern
class ReportBuilder:
    """Builder for generating benchmark reports with fluent interface."""

    def __init__(self, test_data: dict):
        self.test_data = test_data
        self.grouped_data = None

    def build(self) -> str:
        """Build the final report."""
        if not self.grouped_data:
            self._group_by_module()
        return self._generate_nested_hierarchy(self.grouped_data)

    def _group_by_module(self):
        """Group tests by module hierarchy."""
        main_grouped = {
            "garaga": defaultdict(lambda: defaultdict(list)),
            "contracts": defaultdict(lambda: defaultdict(list)),
        }

        for test_name, test_info in self.test_data.items():
            latest_metrics = test_info.get("latest_metrics", {})
            if not latest_metrics:
                continue

            hierarchical_name = latest_metrics.get("test_name_hierarchical", "")
            category = latest_metrics.get("category", "garaga")
            hierarchy_parts = TestResourceInfo.extract_hierarchy_parts(
                hierarchical_name
            )

            current = main_grouped[category]
            for module in hierarchy_parts[:-1]:
                if module not in current:
                    current[module] = defaultdict(list)
                current = current[module]

            final_module = hierarchy_parts[-1] if hierarchy_parts else "tests"
            if not isinstance(current[final_module], list):
                current[final_module] = []
            current[final_module].append(latest_metrics)

        self.grouped_data = {k: dict(v) for k, v in main_grouped.items() if v}

    def _generate_nested_hierarchy(
        self, grouped_data: dict, current_path: List[str] = None
    ) -> str:
        """Generate nested collapsible hierarchy."""
        if current_path is None:
            current_path = []

        content = ""
        # Sort sections alphabetically by module name
        for module_name, module_data in sorted(grouped_data.items()):
            if isinstance(module_data, list):
                df = self._create_dataframe(module_data)
                table_content = self._generate_table(df)
                content += self._collapsible_section(
                    module_name, table_content, False, len(current_path)
                )
            elif isinstance(module_data, dict):
                merged_name, current_data = self._merge_single_children(
                    module_name, module_data
                )

                if isinstance(current_data, list):
                    df = self._create_dataframe(current_data)
                    table_content = self._generate_table(df)
                    content += self._collapsible_section(
                        merged_name, table_content, False, len(current_path)
                    )
                else:
                    inner_content = self._generate_nested_hierarchy(
                        current_data, current_path + [module_name]
                    )
                    is_open = len(current_path) > 0
                    content += self._collapsible_section(
                        merged_name, inner_content, is_open, len(current_path)
                    )

        return content

    def _merge_single_children(self, name: str, data: dict) -> Tuple[str, any]:
        """Merge chains of single children."""
        merged_name = name
        current_data = data

        while len(current_data) == 1:
            child_name = list(current_data.keys())[0]
            child_data = list(current_data.values())[0]

            if isinstance(child_data, list):
                merged_name += "::" + child_name
                return merged_name, child_data
            elif isinstance(child_data, dict):
                merged_name += "::" + child_name
                current_data = child_data
            else:
                break

        return merged_name, current_data

    def _create_dataframe(self, tests: List[dict]) -> pd.DataFrame:
        """Create DataFrame from test data."""
        if not tests:
            return pd.DataFrame()

        columns = [
            "test_name",
            "steps",
            "range_check",
            "range_check96",
            "bitwise",
            "poseidon",
            "add_mod",
            "mul_mod",
            "keccak",
            "pedersen",
            "ecop",
            "syscalls_count",
            "sierra_gas",
            "image_path",
        ]

        df = pd.DataFrame(tests)
        available_columns = [col for col in columns if col in df.columns]
        df = df[available_columns]

        # Remove zero columns except essentials
        essential = ["test_name", "steps", "sierra_gas", "image_path"]
        for col in df.columns:
            if col not in essential and (df[col] == 0).all():
                df = df.drop(columns=[col])

        if "sierra_gas" in df.columns:
            df = df.sort_values("sierra_gas", ascending=False)

        return df

    def _generate_table(self, df: pd.DataFrame) -> str:
        """Generate markdown table."""
        if df.empty:
            return "*No tests found.*\n"

        df = df.copy()

        if "test_name" in df.columns:
            df["test_name"] = df.apply(
                lambda row: f"[{row['test_name']}]({row.get('image_path', 'docs/benchmarks/' + row['test_name'] + '.png')})",
                axis=1,
            )

        # Format numbers
        numeric_cols = [
            "steps",
            "range_check",
            "range_check96",
            "bitwise",
            "poseidon",
            "add_mod",
            "mul_mod",
            "keccak",
            "pedersen",
            "ecop",
            "syscalls_count",
            "sierra_gas",
        ]
        for col in numeric_cols:
            if col in df.columns:
                df[col] = df[col].apply(lambda x: f"{x:,}" if pd.notna(x) else "0")

        # Rename columns
        column_mapping = {
            "test_name": "Test Name",
            "steps": "Steps",
            "range_check": "Range Check",
            "range_check96": "Range Check 96",
            "bitwise": "Bitwise",
            "poseidon": "Poseidon",
            "add_mod": "Add Mod",
            "mul_mod": "Mul Mod",
            "keccak": "Keccak",
            "pedersen": "Pedersen",
            "ecop": "ECOP",
            "syscalls_count": "Syscalls",
            "sierra_gas": "Sierra Gas",
        }

        display_df = df.drop(columns=["image_path"], errors="ignore")
        df_display = display_df.rename(columns=column_mapping)

        return (
            tabulate(df_display, headers="keys", tablefmt="github", showindex=False)
            + "\n"
        )

    def _collapsible_section(
        self, title: str, content: str, is_open: bool, indent_level: int
    ) -> str:
        """Generate collapsible section."""
        open_attr = " open" if is_open else ""

        if indent_level > 0:
            base_indent = "│   " * (indent_level - 1)
            branch = "└── "
            title = f"{base_indent}{branch}{title}"

        return f"""<details{open_attr}>
<summary><strong>{title}</strong></summary>

{content}
</details>

"""


# Workflow Executor
class WorkflowExecutor:
    """Executes command chains with automatic rollback."""

    def __init__(self, config: Config, paths: Paths):
        self.config = config
        self.paths = paths
        self.executed_commands = []

    def execute_workflow(
        self,
        test_filter: Optional[str] = None,
        parallel_jobs: int = 2,
        generate_benchmarks: bool = False,
    ) -> bool:
        """Execute the complete workflow."""
        try:
            self.paths.ensure_dirs()

            commands = [
                CheckDependenciesCommand(self.config),
                RunTestsCommand(self.paths, test_filter),
            ]

            for cmd in commands:
                if not cmd.can_execute():
                    raise Exception(f"Cannot execute {cmd.__class__.__name__}")

                if not cmd.execute():
                    raise Exception(f"Command {cmd.__class__.__name__} failed")

                self.executed_commands.append(cmd)

            # Get passed tests from RunTestsCommand
            run_tests_cmd = next(
                cmd
                for cmd in self.executed_commands
                if isinstance(cmd, RunTestsCommand)
            )

            # Process traces
            process_cmd = ProcessTracesCommand(
                self.paths, run_tests_cmd.passed_tests, parallel_jobs
            )
            if not process_cmd.can_execute():
                raise Exception("Cannot execute ProcessTracesCommand")

            if not process_cmd.execute():
                raise Exception("ProcessTracesCommand failed")

            self.executed_commands.append(process_cmd)

            # Generate benchmarks if requested
            if generate_benchmarks:
                benchmark_cmd = GenerateBenchmarksCommand(self.paths)
                if benchmark_cmd.can_execute():
                    if not benchmark_cmd.execute():
                        raise Exception("GenerateBenchmarksCommand failed")

            Logger.header("🎯 Workflow Complete")
            Logger.log("Profiling workflow completed successfully!", level="success")

            return True

        except KeyboardInterrupt:
            # Re-raise KeyboardInterrupt to be handled by main()
            raise
        except Exception as e:
            import traceback

            Logger.log(f"FATAL ERROR in execute_workflow: {e}", level="error")
            Logger.log(f"Full traceback:\n{traceback.format_exc()}", level="error")
            Logger.exit_panel(
                f"💥 Fatal error in execute_workflow: {e}", "Error", "red"
            )
            sys.exit(1)


# Simplified Main Function & CLI
@dataclass
class CLIConfig:
    """CLI configuration."""

    test_filter: Optional[str] = None
    workspace: Optional[str] = None
    parallel_jobs: int = 2
    generate_benchmarks: bool = False
    benchmarks_only: bool = False


def main():
    """Simplified main entry point."""
    Logger.header("🐺 Garaga Test Profiler")

    # Parse arguments
    parser = argparse.ArgumentParser(description="Run snforge tests with profiling")
    parser.add_argument("test_filter", nargs="?", help="Test name filter")
    parser.add_argument("--all", action="store_true", help="Run all tests")
    parser.add_argument("--workspace", help="Workspace root path")
    parser.add_argument("--parallel-jobs", type=int, default=2, help="Parallel jobs")
    parser.add_argument(
        "--generate-benchmarks", action="store_true", help="Generate benchmarks"
    )
    parser.add_argument(
        "--benchmarks-only", action="store_true", help="Only generate benchmarks"
    )

    args = parser.parse_args()

    # Create CLI config
    cli_config = CLIConfig(
        test_filter=None if args.all else args.test_filter,
        workspace=args.workspace,
        parallel_jobs=max(1, min(8, args.parallel_jobs)),
        generate_benchmarks=args.generate_benchmarks,
        benchmarks_only=args.benchmarks_only,
    )

    # Setup paths and config
    workspace_root = Path(cli_config.workspace or os.getcwd())
    paths = Paths(workspace_root)
    config = Config()

    # Handle benchmarks-only mode
    if cli_config.benchmarks_only:
        Logger.log("Running in benchmarks-only mode", level="info")
        cmd = GenerateBenchmarksCommand(paths)
        success = cmd.execute() if cmd.can_execute() else False

        if success:
            Logger.exit_panel(
                "📊 Cairo benchmarks generated successfully!", "Success", "green"
            )
            sys.exit(0)
        else:
            Logger.exit_panel("💥 Failed to generate Cairo benchmarks!", "Error", "red")
            sys.exit(1)

    # Log configuration
    if cli_config.test_filter:
        Logger.log(f"Running tests matching: {cli_config.test_filter}", level="info")
    else:
        Logger.log("Running all tests", level="info")
    Logger.log(f"Using {cli_config.parallel_jobs} parallel jobs", level="info")

    # Execute workflow
    executor = WorkflowExecutor(config, paths)
    try:
        success = executor.execute_workflow(
            cli_config.test_filter,
            cli_config.parallel_jobs,
            cli_config.generate_benchmarks,
        )
    except KeyboardInterrupt:
        Logger.exit_panel(
            "👋 Process interrupted by user\nPartial results may be available",
            "Interrupted",
            "yellow",
        )
        sys.exit(130)

    # Handle results
    if success:
        Logger.exit_panel(
            "🎉 All operations completed successfully!", "Success", "green"
        )
        sys.exit(0)
    else:
        Logger.exit_panel("💥 Workflow failed!", "Error", "red")
        sys.exit(1)


if __name__ == "__main__":
    main()
